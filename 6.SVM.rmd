---
title: "6. SVM"
author: "Shy-Ann Moehamatdjalil"
date: "19-5-2021"
output: html_document
---

# DEPENDS ON: Data/comment_overview_CLEAN.xlsx, Data/Extra analyse/comment_overview_CLEAN_ZT.xlsx

# CREATES: Data/svm_dataset.csv, Data/Prediction files/5. Support Vector Machine Prediction.xlsx, 
#          Data/Extra analyse/Prediction files/5. Support Vector Machine Prediction_ZT.xlsx

## Load packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(readxl)
library(writexl)
library(caret)
library(LiblineaR)
```

## Load Data
```{r}
# Load in the data
comment_overview_V1 <- read_excel("./Data/comment_overview_CLEAN.xlsx", col_types = "text") 

# Check head of dataframe
comment_overview_V1 %>% head(10)
```

## Divide data intro train and test set

```{r}
# Split into 80/20 for training and test set
set.seed(3456)
trainIndex <- createDataPartition(comment_overview_V1$ma_sentiment, p = .8,
                                  list = FALSE,
                                  times = 1)
training_set <- comment_overview_V1[trainIndex,]
test_set <- comment_overview_V1[-trainIndex,]
```

```{r}
# Write file to csv to use for python 
index <- trainIndex[,1]

svm_dataset <- comment_overview_V1 %>% 
  mutate(training = case_when(
    row_number() %in% index ~ TRUE,
    TRUE ~ FALSE
  ))

svm_dataset %>% 
  write_csv("./Data/svm_dataset.csv")
```

Let's have a look at the distribution of the sentiment categories for both training and test set

```{r}
training_set %>% 
  group_by(ma_sentiment) %>% 
  summarise(count = n()) %>% 
  mutate(percentage = round(count/sum(count) * 100, 2))

test_set %>% 
  group_by(ma_sentiment) %>% 
  summarise(count = n()) %>% 
  mutate(percentage = round(count/sum(count) * 100, 2))
```

## Prepare dataset for analysis

```{r}
# Define a function to calculate the tf_idf features
to_tf_idf <- . %>% 
  unnest_tokens(token, comment_lemma, token = "words", to_lower = TRUE) %>% 
  count(ma_sentiment, sk_id, token) %>% 
  bind_tf_idf(token, sk_id, n) %>% 
  # Words that are not present are NA's but should be 0
  replace_na(list(tf = 0, idf = Inf, tf_idf = 0))
```

```{r}
# Use the function to calculate tf_idf features and make it a sparse matrix for model training
X <- svm_dataset %>% 
  to_tf_idf() %>% 
  cast_sparse(sk_id, token, tf_idf) %>% 
  # Remove rows that do not belong to cases
  .[!is.na(rownames(.)),]

# Pull the target values (these are the sentiment categories)
Y <- factor(svm_dataset$ma_sentiment)

# Based on the index made earlier divide the matrix in train and test set
X_train <- X[index,]
X_test <- X[-index,]
Y_train <- Y[index]
Y_test <- Y[-index]

cat("rows, columns: ", dim(X_train))
cat("rows, columns: ", dim(X_test))
```

### Train the model using LiblineaR package

```{r}
# Center and scale  train data
s_train <- scale(X_train, center = TRUE, scale = TRUE)
#Replace NA for 0
s_train[is.na(s_train)] <- 0
```

```{r}
# Estimate the value for the C constant
co <- heuristicC(s_train)
```

```{r, cache=TRUE}
# Find the best model with the best cost parameter via 5-fold cross-validations
tryTypes <- c(1:3) 
bestAcc <-  0
bestType <-  NA
for(ty in tryTypes){
  acc = LiblineaR(data = s_train, target = Y_train, type = ty, cost = co, bias = 1, cross = 5, verbose = FALSE)
  cat("Results for C=", co," : ", acc," accuracy.\n", sep = "")
  if(acc > bestAcc){
    bestAcc = acc
    bestType = ty
  }
}

cat("Best model type is:",bestType,"\n")
cat("Best accuracy is:",bestAcc,"\n")

```

### Make predictions based on best model

```{r}
# Re-train best model with best cost value.
svm_model <- LiblineaR(data = s_train, target = Y_train, type = bestType, cost = co, bias = 1, verbose = FALSE)

# Center and scale test data
s_test <- scale(X_test,attr(s_train,"scaled:center"),attr(s_train,"scaled:scale"))

#Replace NA and Inf for 0
s_test[is.na(s_test)] <- 0
s_test[is.infinite(s_test)] <- 0
```

```{r}
# Make predictions
predictions <- predict(svm_model, s_test, decisionValues = TRUE)

# Display confusion matrix
res <- table(Y_test, predictions$predictions)
res
prop.table(res) %>% round(2)

# Calculate accuracy
accuracy <- sum(diag(res))/ nrow(s_test) * 100
accuracy %>% round(2)
```

### Add predictions to the datafile
```{r}
test_set$predicted <- predictions$predictions
test_set

# Make a frequency table that compares manual and automated
sentiment_scoretable <- table(test_set$ma_sentiment, test_set$predicted)
names(dimnames(sentiment_scoretable)) <- c("MANUAL", "AUTOMATED")

# Cross table with the different categories
sentiment_scoretable

# Cross table with proportions
prop.table(sentiment_scoretable) %>% 
  round(2)

# Cross table with the different categories, in row percentages 
prop.table(sentiment_scoretable, 1) %>% 
  round(2)

```

### Write predictions to file

```{r}
test_set %>% 
  write_xlsx("./Data/Prediction files/5. Supper Vector Machine Prediction.xlsx")
```

### EXTRA ANALYSIS ####

Same analysis but this time without the comments that were marked as twijfelgevallen
```{r}
# Read in data
comments_ZT <- read_xlsx("./Data/Extra analyse/comment_overview_CLEAN_ZT.xlsx")

## Divide data intro train and test set
# Split into 80/20 for training and test set
set.seed(3456)
trainIndex_ZT <- createDataPartition(comments_ZT$ma_sentiment, p = .8,
                                  list = FALSE,
                                  times = 1)
training_set_ZT <- comments_ZT[trainIndex_ZT,]
test_set_ZT <- comments_ZT[-trainIndex_ZT,]

index_ZT <- trainIndex_ZT[,1]

# Use the function to calculate tf_idf features and make it a sparse matrix for model training
X_ZT <- comments_ZT %>% 
  to_tf_idf() %>% 
  cast_sparse(sk_id, token, tf_idf) %>% 
  # Remove rows that do not belong to cases
  .[!is.na(rownames(.)),]

# Pull the target values (these are the sentiment categories)
Y_ZT <- factor(comments_ZT$ma_sentiment)

# Based on the index made earlier divide the matrix in train and test set
X_train_ZT <- X_ZT[index_ZT,]
X_test_ZT <- X_ZT[-index_ZT,]
Y_train_ZT <- Y_ZT[index_ZT]
Y_test_ZT <- Y_ZT[-index_ZT]

cat("rows, columns: ", dim(X_train_ZT))
cat("rows, columns: ", dim(X_test_ZT))

### Train the model using LiblineaR package
# Center and scale  train data
s_train_ZT <- scale(X_train_ZT, center = TRUE, scale = TRUE)
#Replace NA for 0
s_train_ZT[is.na(s_train_ZT)] <- 0
# Estimate the value for the C constant
co_ZT <- heuristicC(s_train_ZT)

### Make predictions based on best model
# Re-train best model with best cost value.
svm_model_ZT <- LiblineaR(data = s_train_ZT, target = Y_train_ZT, type = bestType, cost = co_ZT, bias = 1, verbose = FALSE)

# Center and scale test data
s_test_ZT <- scale(X_test_ZT,attr(s_train_ZT,"scaled:center"),attr(s_train_ZT,"scaled:scale"))

#Replace NA and Inf for 0
s_test_ZT[is.na(s_test_ZT)] <- 0
s_test_ZT[is.infinite(s_test_ZT)] <- 0
  
# Make predictions
predictions_ZT <- predict(svm_model_ZT, s_test_ZT, decisionValues = TRUE)

# Display confusion matrix
res_ZT <- table(Y_test_ZT, predictions_ZT$predictions)
res_ZT
prop.table(res_ZT) %>% round(2)

# Calculate accuracy
accuracy_ZT <- sum(diag(res_ZT))/ nrow(s_test_ZT) * 100
accuracy_ZT %>% round(2)

### Add predictions to the datafile
test_set_ZT$predicted <- predictions_ZT$predictions
test_set_ZT

# Make a frequency table that compares manual and automated
sentiment_scoretable_ZT <- table(test_set_ZT$ma_sentiment, test_set_ZT$predicted)
names(dimnames(sentiment_scoretable_ZT)) <- c("MANUAL", "AUTOMATED")

# Cross table with the different categories
sentiment_scoretable_ZT

# Cross table with proportions
prop.table(sentiment_scoretable_ZT) %>% 
  round(2)

# Cross table with the different categories, in row percentages 
prop.table(sentiment_scoretable_ZT, 1) %>% 
  round(2)


### Write predictions to file
test_set_ZT %>% 
  write_xlsx("./Data/Extra analyse/Prediction files/5. Support Vector Machine Prediction_ZT.xlsx")
```


